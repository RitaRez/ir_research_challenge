{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qrels é o arquivo que carrega a relevância que uma certa entidade, ou documento, tem para uma consulta. No nosso caso 0: nada relevante, 1: pouco relevante e 2: bastante relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QueryId</th>\n",
       "      <th>EntityId</th>\n",
       "      <th>Relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>367937</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>429992</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>513435</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>571751</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>582040</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   QueryId  EntityId  Relevance\n",
       "0        9    367937          1\n",
       "1        9    429992          2\n",
       "2        9    513435          1\n",
       "3        9    571751          2\n",
       "4        9    582040          2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels = pd.read_csv('../data/train_qrels.csv')\n",
    "qrels.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features seria o score que cada um dos modelos que a gente implementar da pra um par (Consulta, Documento). Então por exemplo, uma feature pode ser BM25, outra TFIDF, outra Dense Retrieval e por aí vai. O Learning to Rank (se tudo der certo) vai aprender a dar relevancia para cada uma dessas features extraindo um pouquinho (e talvez o melhor) de cada uma.\n",
    "\n",
    "Agora parando pra pensar esse merge vai ser bem mais complicado do q eu tinha imaginado de primeira, pq tem elements q não vão estar presentes em alguns casos, nesses casos acho que eu vou só por 0 nessa célula. Se eu for retornar os 100 melhores de cada feature e eu usar 5 features, pode acontecer de ter 500 amostras por consulta (exagero da minha parte), vou ter que analisar isso com cuidado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_paths = [\"../results/bm25_output/\", \"../results/tf_idf_output/\"]\n",
    "\n",
    "features_df\n",
    "for feature in features_paths:\n",
    "    current_feature = pd.read_csv(feature)\n",
    "    features_df = pd.concat([feature_df, current_feature], axis=1, ignore_index=True)\n",
    "\n",
    "features_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a gente vai juntar nossas features com a relevancia do gabarito (na pratica só pra ter um negócio bonito pra ver...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltr_df = pd.merge(qrels, features_df,  how='left', left_on=['QueryId','EntityId'])\n",
    "ltr_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É melhor eu escrever isso se não vou esquecer...\n",
    "Primeiro eu obtenho os ids de todas as consultas de teste dando um ```unique()``` na coluna de ids e armazendo em ```querie_ids```. Em seguida acho a quantidade de ids únicos e armazeno em ```amount_of_queries```. <br>\n",
    "\n",
    "Minha proporção treino validação vai ser 80/20, então o tanto de consultas no treino vai ser ```train_size = 0.8 * amount_of_queries``` e o tanto de consultas no val  ```0.2 * amount_of_queries``` (como tenho quantidades de sample para consulta variaveis o buraco é mais embaixo). \n",
    "\n",
    "Agora vou achar a consulta que vai ser o ponto de \"corte\" fazendo ```cut = queries_ids[train_size]```. Assim para achar o df de treino basta selecionar todas as rows cujo ```QueryId > cut```, o restante será o teste.\n",
    "\n",
    "\n",
    "Obs: Não existe amostra de uma mesma consulta em treino e teste. Fiz isso propositalmente pois não tinha certeza se isso se enquadrava em data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'querie_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m queries_ids \u001b[39m=\u001b[39m qrels[\u001b[39m'\u001b[39m\u001b[39mQueryId\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()\n\u001b[1;32m----> 2\u001b[0m amount_of_queries \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(querie_ids)\n\u001b[0;32m      4\u001b[0m train_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m0.8\u001b[39m \u001b[39m*\u001b[39m amount_of_queries)\n\u001b[0;32m      5\u001b[0m val_size \u001b[39m=\u001b[39m amount_of_queries \u001b[39m-\u001b[39m train_size\n",
      "\u001b[1;31mNameError\u001b[0m: name 'querie_ids' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "queries_ids = qrels['QueryId'].unique()\n",
    "amount_of_queries = len(querie_ids)\n",
    "\n",
    "train_size = int(0.8 * amount_of_queries)\n",
    "val_size = amount_of_queries - train_size\n",
    "\n",
    "cut = querie_ids[train_size]\n",
    "train_df = ltr_df[ltr_df['QueryId'] <= cut]\n",
    "val_df = ltr_df[ltr_df['QueryId'] > cut]\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguinte, o formato para o lgbm é confuso, mas na pratica x_train é as features, y_train vai ser a relevancia e group vai dividir todas as amostras pela query a qual essa se refere. Então se a primeira query tem 10 samples, a segunda 12, a terceira 14, e a quinta 16, ```group = [10, 12, 14, 16]```. Veja bem, nesse exemplo hipotetico ```x_train``` possui 10 linhas referentes a primeira query, 12 referente a segunda... e a mesma coisa para ```y_val```!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qids_train = train_df.groupby(\"QueryId\")[\"QueryId\"].count().to_numpy()\n",
    "X_train = train_df.drop([\"QueryId\", \"Relevance\"], axis=1)\n",
    "y_train = train_df[\"Relevance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qids_validation = val_df.groupby(\"QueryId\")[\"QueryId\"].count().to_numpy()\n",
    "X_validation = val_df.drop([\"QueryId\", \"Relevance\"], axis=1)\n",
    "y_validation = val_df[\"Relevance\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora é instanciar o LightGMB, tem um monte de hiperparametro e _acho_ que esse [link](https://neptune.ai/blog/lightgbm-parameters-guide) aqui tem uma explicação que vai ser ótima pro futuro..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lightgbm.LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    group=qids_train,\n",
    "    eval_set=[(X_validation, y_validation)],\n",
    "    eval_group=[qids_validation],\n",
    "    eval_at=10,\n",
    "    verbose=10,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora é que são elas, até onde eu entendo eu vou ter q gerar as features todas para o teste (arquivo separado) e mergear ela (não parece trivial), e aí passar pra função abaixo (para cada consulta (???)). Cada row vai ser uma entidade né, então vou ter um monte de score para cada entidade, aí é ordenar pivotando para ter os ids ordenados tb e retornar esses ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = gbm.predict(X_test)\n",
    "X_test[\"predicted_ranking\"] = test_pred\n",
    "X_test.sort_values(\"predicted_ranking\", ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referências\n",
    "\n",
    "- https://tamaracucumides.medium.com/learning-to-rank-with-lightgbm-code-example-in-python-843bd7b44574\n",
    "- https://towardsdatascience.com/how-to-evaluate-learning-to-rank-models-d12cadb99d47\n",
    "- https://towardsdatascience.com/how-to-implement-learning-to-rank-model-using-python-569cd9c49b08\n",
    "- https://stackoverflow.com/questions/64294962/how-to-implement-learning-to-rank-using-lightgbm\n",
    "- https://www.kaggle.com/code/bturan19/lightgbm-ranker-introduction/notebook\n",
    "- https://stackoverflow.com/questions/62555987/lightgbm-ranking-example/67621253#67621253\n",
    "- https://neptune.ai/blog/lightgbm-parameters-guide\n",
    "- https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRanker.html#\n",
    "- https://github.com/uni-assignments/research-challenge-2/blob/master/src/ltr.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
